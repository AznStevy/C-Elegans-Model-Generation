{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "%reset\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import interpolate\n",
    "from thin_plate_spline_warp import thin_plate_spline_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     13,
     30
    ]
   },
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "def create_workspace_folders():\n",
    "    date = datetime.datetime.now()\n",
    "    workspace = \"workspace\\{}-{}\".format(date.strftime(\"%Y_%m_%d\"), name)\n",
    "\n",
    "    create_folders = ['workspace', workspace]\n",
    "    # create workspace\n",
    "    for folder in create_folders:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "    \n",
    "    return workspace\n",
    "\n",
    "def moving_average(a, window_size):\n",
    "    new_a = []\n",
    "    half_window = round(window_size/2)\n",
    "    for idx, val in enumerate(a):\n",
    "        if val == 0:\n",
    "            new_a.append(0)\n",
    "            continue\n",
    "            \n",
    "        start_idx = max(idx-half_window, 0)\n",
    "        end_idx = min(idx+half_window, a.shape[0]-1)\n",
    "        a_segment = a[start_idx:end_idx]\n",
    "        \n",
    "        a_seg_mean = np.nanmean(np.where(a_segment!=0, a_segment, np.nan)) # nonzero mean\n",
    "        new_a.append(a_seg_mean)\n",
    "        \n",
    "    return new_a\n",
    "\n",
    "def hampel_filter(input_series, window=5, n_sigmas=1):\n",
    "    # ensure the data is flattened\n",
    "    input_series = np.array(input_series).flatten()\n",
    "    \n",
    "    # returns filtered timepoints and coordinates along\n",
    "    n = len(input_series)\n",
    "    k = 1.4826 # scale factor for Gaussian distribution\n",
    "    \n",
    "    outliers_idxs = []\n",
    "    outliers_filtered = []\n",
    "    extended_series = np.pad(input_series, (window, window), 'reflect')\n",
    "    filtered_series = extended_series.copy()\n",
    "    n_ex = len(extended_series)\n",
    "    for i in range(window_size, n_ex - window_size):\n",
    "        x0 = np.median(\n",
    "            extended_series[(i - window_size):(i + window_size)])\n",
    "        S0 = k * np.median(np.abs(\n",
    "            extended_series[(i - window_size):(i + window_size)] - x0))\n",
    "        if (np.abs(extended_series[i] - x0) > n_sigmas * S0):\n",
    "            filtered_series[i] = x0 # replaces outlier with median\n",
    "            outliers_idxs.append(i-window) # logs outlier index\n",
    "            outliers_filtered.append(x0) # the value that replaces outlier\n",
    "    \n",
    "    filtered_series = filtered_series[window:len(filtered_series)-window]\n",
    "    \n",
    "    return filtered_series, outliers_idxs, outliers_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"OD1599_NU+NerveRing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace folderpath: workspace\\2020_08_16-OD1599_NU+NerveRing\n"
     ]
    }
   ],
   "source": [
    "# Create workspace folders\n",
    "workspace_folderpath = create_workspace_folders()\n",
    "print('Workspace folderpath: {}'.format(workspace_folderpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load necessary file structures via config file\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1,
     163,
     186,
     209
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\OD1599_NU\\OD1599_MostRecent\\120619_Pos2\\Decon_reg\n",
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\OD1599_NU\\OD1599_MostRecent\\112719_Pos3\\Decon_Reg\n",
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\OD1599_NU\\OD1599_MostRecent\\112619_Pos0\\Decon_reg\n",
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\DCR6485_RPM1_NU\\011419_Pos0\\Decon_reg\n",
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\DCR6485_RPM1_NU\\011419_Pos4\\Decon_reg\n",
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\DCR6485_RPM1_NU\\021020_Pos2\\Decon_Reg\n",
      "DATA ERROR: Ignoring identical cell IDs (A3) for cell nr_3 in Y:\\RyanC\\Cell Tracking Project\\DCR6485_RPM1_NU\\021020_Pos2\\Decon_Reg\\RegB\\Decon_reg_8\\Decon_reg_8_results\\straightened_annotations\\straightened_annotations.csv\n",
      "DATA ERROR: Ignoring identical cell IDs (A7) for cell nr_7 in Y:\\RyanC\\Cell Tracking Project\\DCR6485_RPM1_NU\\021020_Pos2\\Decon_Reg\\RegB\\Decon_reg_16\\Decon_reg_16_results\\straightened_annotations\\straightened_annotations.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import all the necessary data\n",
    "def parse_mipav_data(config, cell_key, strain_folderpath):\n",
    "    # define output variables\n",
    "    valid_seam_cells = set(config['settings']['interpolation']['seam_cells_on'].keys())\n",
    "    seam_cell_output = {}\n",
    "    annotation_output = {} # structured cell_name: pandas dataframe (timepoint, x, y, z)\n",
    "    errors = [] # note potential errors in volumes\n",
    "    \n",
    "    # determine default or options specific to strain\n",
    "    folderpaths = config['settings']['folderpaths']\n",
    "    for folder_data in folderpaths.keys():\n",
    "        try:\n",
    "            folderpaths[folder_data] = cell_key['folderpaths'][folder_data]\n",
    "            print('Using specific {} folder structure'.format(folder_data))\n",
    "        except:\n",
    "            # print('Using default {} folder structure'.format(folder_data))\n",
    "            pass\n",
    "    \n",
    "    # find the numbers of the folders\n",
    "    start_vol = int(cell_key['start'])\n",
    "    end_vol = int(cell_key['end'])\n",
    "    all_folderpaths = folderpaths.copy()\n",
    "    for vol_idx, vol_num in enumerate(range(start_vol, end_vol+1)):\n",
    "        # define the specific volume number folder\n",
    "        vol_folder = all_folderpaths['data_folderpath'].replace('#', str(vol_num))\n",
    "        vol_folderpath = os.path.join(\n",
    "            strain_folderpath, all_folderpaths['side'], vol_folder)\n",
    "        \n",
    "        # check if the folder exists\n",
    "        if not os.path.isdir(vol_folderpath):\n",
    "            continue\n",
    "        \n",
    "        # define individual, full filepaths from drive\n",
    "        all_filepaths_folderpaths = {}\n",
    "        for full_folderpaths_key in all_folderpaths.keys():\n",
    "            if full_folderpaths_key != \"side\" and \\\n",
    "                full_folderpaths_key != \"data_folderpath\":\n",
    "                all_filepaths_folderpaths[full_folderpaths_key] = os.path.join(\n",
    "                    vol_folderpath, all_folderpaths[full_folderpaths_key])\n",
    "                # print(all_filepaths_folderpaths[full_folderpaths_key])\n",
    "\n",
    "                # check to see if exists\n",
    "                if not os.path.isfile(all_filepaths_folderpaths[full_folderpaths_key]):\n",
    "                    error_msg = \"FILE DOES NOT EXIST: {}\".format(\n",
    "                        all_filepaths_folderpaths[full_folderpaths_key])\n",
    "                    errors.append(error_msg)\n",
    "                    print(error_msg)\n",
    "        \n",
    "        ## get individual data by cell in pandas format\n",
    "        try:\n",
    "            # import twisted seam cells           \n",
    "            tw_seam_cells_fp = all_filepaths_folderpaths['twisted_seam_cells']\n",
    "            tw_seam_cells = pd.read_csv(tw_seam_cells_fp)            \n",
    "            # import twisted annotations\n",
    "            tw_annotations_fp = all_filepaths_folderpaths['twisted_annotations']\n",
    "            tw_annotations = pd.read_csv(tw_annotations_fp)\n",
    "            # import straighted seam cells\n",
    "            st_seam_cells_fp = all_filepaths_folderpaths['straightened_seam_cells']\n",
    "            st_seam_cells = pd.read_csv(st_seam_cells_fp)\n",
    "            # import straightened annotations\n",
    "            st_annotations_fp = all_filepaths_folderpaths['straightened_annotations']\n",
    "            st_annotations = pd.read_csv(st_annotations_fp)\n",
    "        except:\n",
    "            continue # file errors should already be logged.\n",
    "            \n",
    "        # ERROR CHECKING: Check for file content ----------------------\n",
    "        if st_seam_cells.size == 0: # check if empty\n",
    "            error_msg = \"DATA ERROR: Empty seam cell file at {}\".format(st_seam_cells_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        if st_annotations.size == 0: # check if empty\n",
    "            error_msg = \"DATA ERROR: Empty annotation file at {}\".format(st_annotations_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        data_file_list = [ # use to check values of files\n",
    "            (tw_seam_cells, tw_seam_cells_fp), (tw_annotations, tw_annotations_fp),\n",
    "            (st_seam_cells, st_seam_cells_fp), (st_annotations, st_annotations_fp)\n",
    "        ]\n",
    "        for data, fp in data_file_list:\n",
    "            coordinates = data[['x_voxels', 'y_voxels', 'z_voxels']]\n",
    "            cell_ids = data['name'].values.tolist()\n",
    "            for cell_idx, cell_coord in coordinates.iterrows():\n",
    "                if not cell_coord.equals(cell_coord.astype(float)):\n",
    "                    error_msg = \"DATA ERROR: Non-float found for cell {} in {}\".format(\n",
    "                        cell_names[cell_idx], fp)\n",
    "                    errors.append(error_msg)\n",
    "                    print(error_msg)            \n",
    "        seam_cells_mismatch = list(set(st_seam_cells['name'].values.tolist()) - \n",
    "                                   set(tw_seam_cells['name'].values.tolist()))\n",
    "        annotations_mismatch = list(set(st_annotations['name'].values.tolist()) - \n",
    "                                   set(tw_annotations['name'].values.tolist())) \n",
    "        if seam_cells_mismatch:\n",
    "            error_msg = \"DATA ERROR: Mis-match between seam cells ({}) found in {} and {}\".format(\n",
    "                \", \".join(seam_cells_mismatch), st_seam_cells_fp, tw_seam_cells_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        if annotations_mismatch:\n",
    "            error_msg = \"DATA ERROR: Mis-match between annotated cells ({}) found in {} and {}\".format(\n",
    "                \", \".join(annotations_mismatch), st_annotations_fp, tw_annotations_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        # check if extra seam cells exist/mismatch with settings\n",
    "        extra_seam_cells = list(set(st_seam_cells['name'].values.tolist()) - valid_seam_cells)\n",
    "        if extra_seam_cells: # check if there are extra seam cells\n",
    "            # save old one as _bak.csv, save new as straightened_seamcells.csv\n",
    "            # st_seam_cells.to_csv(st_seam_cells_fp.replace('.csv', '_bak.csv'), index=False)\n",
    "            for extra_seam_cell in extra_seam_cells:\n",
    "                st_seam_cells = st_seam_cells[st_seam_cells['name'] != extra_seam_cell]\n",
    "            # st_seam_cells.to_csv(st_seam_cells_fp, index=False)\n",
    "            error_msg = \"DATA WARNING: Ignoring extra seam cells ({}) found in {}\".format(\n",
    "                \", \".join(extra_seam_cells), st_seam_cells_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        # END FILE ERROR CHECK -----------------------------------------------------------\n",
    "\n",
    "        # SEAM CELLS: combine into a single data structure and catch data errors\n",
    "        # go through each row of the file and log the appropriate information\n",
    "        for idx, seam_row in st_seam_cells.iterrows():\n",
    "            seam_cell_name = seam_row['name'].upper() # seam cells are upper case? just stay consistent\n",
    "            if seam_cell_name not in seam_cell_output.keys():\n",
    "                seam_cell_output[seam_cell_name] = {}\n",
    "                seam_cell_output[seam_cell_name]['timepoints'] = []\n",
    "                seam_cell_output[seam_cell_name]['coordinates'] = []\n",
    "\n",
    "            # append to appropriate seam cell\n",
    "            if vol_num not in seam_cell_output[seam_cell_name]['timepoints']:\n",
    "                seam_cell_coords = seam_row[['x_voxels','y_voxels','z_voxels']].values.flatten().tolist()\n",
    "                seam_cell_output[seam_cell_name]['coordinates'].append(seam_cell_coords)\n",
    "                seam_cell_output[seam_cell_name]['timepoints'].append(vol_num)\n",
    "\n",
    "        # ANNOTATIONS: combine into a single data structure and catch data errors\n",
    "        try:\n",
    "            for cell_id in cell_key['mapping'].keys():\n",
    "                cell_name = cell_key['mapping'][cell_id].lower()\n",
    "                if cell_name not in annotation_output.keys():\n",
    "                    annotation_output[cell_name] = {}\n",
    "                    annotation_output[cell_name]['timepoints'] = []\n",
    "                    annotation_output[cell_name]['coordinates'] = []\n",
    "\n",
    "                # check if the cell name exists in volume\n",
    "                # print(vol_folderpath, st_annotations['name'], cell_id)\n",
    "                cell_row = st_annotations.loc[st_annotations['name'] == cell_id]\n",
    "                if not cell_row.empty:\n",
    "                    # try to catch a few errors\n",
    "                    if cell_row.shape[0] > 1:\n",
    "                        error_msg = \"DATA ERROR: Ignoring identical cell IDs ({}) for cell {} in {}\".format(\n",
    "                            cell_id, cell_name, st_annotations_fp)\n",
    "                        errors.append(error_msg)\n",
    "                        print(error_msg)\n",
    "                        continue\n",
    "                    \n",
    "                    # if there is nothing wrong, then proceed\n",
    "                    if vol_num not in annotation_output[cell_name]['timepoints']:\n",
    "                        cell_coords = cell_row[['x_voxels','y_voxels','z_voxels']].values.flatten().tolist()\n",
    "                        annotation_output[cell_name]['coordinates'].append(cell_coords)\n",
    "                        annotation_output[cell_name]['timepoints'].append(vol_num)\n",
    "        except:\n",
    "            error_msg = \"DATA ERROR: Failure to read file {}\".format(st_annotations_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "                \n",
    "    return seam_cell_output, annotation_output, errors\n",
    "\n",
    "def convert_cell_key_csv2json(csv_filepath):\n",
    "    cell_key_json = {}\n",
    "    # load in csv file\n",
    "    cell_key = pd.read_csv(csv_filepath, header=None, engine='python')\n",
    "    \n",
    "    ## get necessary information\n",
    "    cell_key_json['name'] = str(cell_key.iloc[0,0])\n",
    "    cell_key_json['start'] = int(cell_key.iloc[1,0])\n",
    "    cell_key_json['end'] = int(cell_key.iloc[1,1])\n",
    "    try:\n",
    "        cell_key_json['outliers'] = cell_key.iloc[2].astype(int).values.tolist()\n",
    "    except:\n",
    "        cell_key_json['outliers'] = [] # if there are none\n",
    "    \n",
    "    # grab mapping\n",
    "    cell_key_json['mapping'] = {}\n",
    "    for row_idx in range(3, cell_key.shape[0]): # mapping starts row idx 3\n",
    "        cell_id = str(cell_key.iloc[row_idx,0])\n",
    "        cell_name = str(cell_key.iloc[row_idx,1])\n",
    "        cell_key_json['mapping'][cell_id] = cell_name\n",
    "\n",
    "    return cell_key_json\n",
    "    \n",
    "def get_cell_key(pos_folderpath):\n",
    "    # return the cell key in dict/json format and convert any existing \n",
    "    # cell key csv into json if the json version doesn't exist.\n",
    "    cell_key_filepath_csv = os.path.join(pos_folderpath, 'CellKey.csv')\n",
    "    cell_key_filepath_json = os.path.join(pos_folderpath, 'cell_key.json')\n",
    "    \n",
    "    # if the json exists, use it\n",
    "    if os.path.isfile(cell_key_filepath_json):\n",
    "        with open(cell_key_filepath_json) as f:\n",
    "            return json.load(f)\n",
    "    elif os.path.isfile(cell_key_filepath_csv):\n",
    "        cell_key_json = convert_cell_key_csv2json(cell_key_filepath_csv)\n",
    "        cell_key_json_dump = json.dumps(cell_key_json, sort_keys=True, indent=4)\n",
    "        with open(cell_key_filepath_json, \"w\") as f: \n",
    "            f.write(cell_key_json_dump)\n",
    "        return cell_key_json\n",
    "    else:\n",
    "        cell_key_error_msg = 'Missing cell key: {}'.format(cell_key_filepath_json)\n",
    "        print(cell_key_error_msg)\n",
    "        return None\n",
    "                \n",
    "strain_info = config['data']['strains']\n",
    "compiled_data = {}\n",
    "for strain in strain_info:\n",
    "    if strain['include']:\n",
    "        if strain['name'] not in compiled_data.keys():\n",
    "            compiled_data[strain['name']] = {}\n",
    "                \n",
    "        for pos_folderpath in strain['folderpaths']:\n",
    "            print('CURRENTLY LOADING: {}'.format(pos_folderpath))\n",
    "            # find and load cell key\n",
    "            cell_key = get_cell_key(pos_folderpath)\n",
    "            if not cell_key:\n",
    "                continue\n",
    "                \n",
    "            # get data\n",
    "            seam_cells, annotations, strain_errors = parse_mipav_data(\n",
    "                config, cell_key, pos_folderpath)\n",
    "            \n",
    "            # combine data\n",
    "            pos_name = cell_key['name']\n",
    "            compiled_data[strain['name']][pos_name] = {}\n",
    "            compiled_data[strain['name']][pos_name]['cell_key'] = cell_key\n",
    "            compiled_data[strain['name']][pos_name]['seam_cells'] = seam_cells\n",
    "            compiled_data[strain['name']][pos_name]['annotations'] = annotations\n",
    "            compiled_data[strain['name']][pos_name]['errors'] = strain_errors\n",
    "\n",
    "# save information as intermediate step in workspace\n",
    "compiled_json = json.dumps(compiled_data, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '1_compiled_data.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 Completed.\n",
      "Step 2 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Check for outliers, uses parameters defined in config.json\n",
    "\n",
    "window_size = config['settings']['outlier_removal']['window_size']\n",
    "n_stdev = config['settings']['outlier_removal']['n_stdev']\n",
    "compiled_data_no_outliers = copy.deepcopy(compiled_data)\n",
    "for strain in compiled_data.keys():\n",
    "    for pos in compiled_data[strain].keys():\n",
    "\n",
    "        # determine outliers for both seam cells and annotations\n",
    "        for cells_type in ['seam_cells', 'annotations']:\n",
    "            for cell in compiled_data[strain][pos][cells_type].keys():\n",
    "                coordinates = np.array(compiled_data[strain][pos][cells_type][cell]['coordinates'])\n",
    "\n",
    "                # separate by x,y,z (index 0-2) and filter\n",
    "                outlier_idxs = {} # set automatically removes repeats\n",
    "                for dim_idx in range(3):\n",
    "                    coord_dim_data = copy.deepcopy(coordinates[:, dim_idx])\n",
    "                    data_filtered, outlier_idx, outlier_val = hampel_filter(coord_dim_data, \n",
    "                            n_sigmas=n_stdev, window=window_size)\n",
    "                    coordinates[:, dim_idx] = data_filtered\n",
    "\n",
    "                # replace time series outliers using median\n",
    "                compiled_data_no_outliers[strain][pos][cells_type][cell]['coordinates'] = coordinates.tolist()\n",
    "\n",
    "print('Step 2 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "compiled_json = json.dumps(compiled_data_no_outliers, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '2_compiled_data_no_outliers.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 2 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 Completed.\n",
      "Step 3 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Interpolate each to appropriate time scale\n",
    "compiled_data_interp = copy.deepcopy(compiled_data_no_outliers)\n",
    "seam_cells_on = config['settings']['interpolation']['seam_cells_on'] # in minutes\n",
    "total_len = config['settings']['interpolation']['total_min'] # in minutes\n",
    "interp_method = config['settings']['interpolation']['method']\n",
    "min_timepoints_required = config['settings']['interpolation']['min_timepoints_required']\n",
    "new_timepoints = np.linspace(0, total_len)\n",
    "\n",
    "for strain in compiled_data_no_outliers.keys():\n",
    "    for pos in compiled_data_no_outliers[strain].keys():\n",
    "        \n",
    "        cell_key = compiled_data_no_outliers[strain][pos]['cell_key']\n",
    "        # interpolate for both seam cells and annotations\n",
    "        for cells_type in ['seam_cells', 'annotations']:\n",
    "            for cell in compiled_data_no_outliers[strain][pos][cells_type].keys():\n",
    "                timepoints = np.array(compiled_data_no_outliers[strain][pos][cells_type][cell]['timepoints'])\n",
    "                coordinates = np.array(compiled_data_no_outliers[strain][pos][cells_type][cell]['coordinates'])\n",
    "                \n",
    "                # CHECKS if there are a sufficient number of timepoints. if not, raise error.\n",
    "                if timepoints.size < min_timepoints_required:\n",
    "                    error_msg = \"DATA ERROR: Insufficient timepoints in {}/{} for cell {} where only {} (volumes {}), less than the required {}, exist.\".format(\n",
    "                        strain, pos, cell, str(len(timepoints)), \n",
    "                        \",\".join(timepoints.astype(str).tolist()), str(min_timepoints_required))\n",
    "                    compiled_data_interp[strain][pos]['errors'].append(error_msg)\n",
    "                    print(error_msg)\n",
    "                    continue\n",
    "                \n",
    "                # handle seam cells and annotations differently because \n",
    "                # some seam cells appear after twitching begins\n",
    "                if cells_type == 'seam_cells':\n",
    "                    # determine if strain starts after designated seam cell on\n",
    "                    seam_cell_on_percent = (min(timepoints) - cell_key['start'])/(cell_key['end'] - cell_key['start']) # find how far in is this timpoint\n",
    "                    seam_cell_on_percent = max(seam_cells_on[cell], seam_cell_on_percent)\n",
    "                    starting_idx = int(round(seam_cell_on_percent * total_len))\n",
    "                    \n",
    "                    # print(len(timepoints), seam_cells_on[cell], seam_cell_on_percent, starting_idx)\n",
    "                    \n",
    "                    # calculate when the cell turns on, doesn't necessarily correspond to index\n",
    "                    og_target = round((cell_key['end'] - cell_key['start']) * seam_cell_on_percent + cell_key['start'])\n",
    "                    og_starting_idx = np.argmin(np.abs(timepoints - og_target)).astype(int) # find closest to target\n",
    "                    \n",
    "                elif cells_type == 'annotations':\n",
    "                    starting_idx = 0\n",
    "                    og_starting_idx = 0\n",
    "                \n",
    "                new_coordinates = np.zeros((total_len, 3))\n",
    "                new_timepoints = np.arange(total_len)\n",
    "                \n",
    "                # interpolate one dimension at a time\n",
    "                for dim_idx in range(3):\n",
    "                    # crop the original data to the appropriate length, e.g. Q: top 25% of volumes\n",
    "                    cell_timepoints = timepoints[og_starting_idx:].copy()\n",
    "                    coord_dim_data = coordinates[og_starting_idx:, dim_idx].copy()\n",
    "                    # rescale time points to range\n",
    "                    cell_sp_rescaled = cell_timepoints - min(cell_timepoints)\n",
    "                    cell_sp_rescaled = cell_sp_rescaled/max(cell_sp_rescaled) * (total_len - starting_idx)\n",
    "                    \n",
    "                    # tck = interpolate.splrep(cell_sp_rescaled, coord_dim_data, s=0)\n",
    "                    \n",
    "                    interp = interpolate.interp1d(cell_sp_rescaled, coord_dim_data,\n",
    "                                     kind=interp_method) # get interp as if from 0, but shift below\n",
    "                    # new time scale for specific length (might be part of total)  \n",
    "                    cell_sp_timepoints = np.arange(total_len - starting_idx)\n",
    "                    # interped_coords = interpolate.splev(cell_sp_timepoints, tck, der=0)\n",
    "                    interped_coords = interp(cell_sp_timepoints)\n",
    "                    new_coordinates[starting_idx:, dim_idx] = interped_coords # shifted\n",
    "\n",
    "                # replace time series outliers using median\n",
    "                compiled_data_interp[strain][pos][cells_type][cell]['coordinates'] = new_coordinates.tolist()\n",
    "                compiled_data_interp[strain][pos][cells_type][cell]['timepoints'] = new_timepoints.tolist()\n",
    "                \n",
    "# save information as intermediate step in workspace\n",
    "print('Step 3 Completed.')\n",
    "compiled_json = json.dumps(compiled_data_interp, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '3_compiled_data_interpolation.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 3 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     7,
     22
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following strains for warping: OD1599_NU_1206_Pos2, OD1599_NU_1127_Pos3, OD1599_NU_1126_Pos0\n",
      "Step 4 Completed.\n",
      "Step 4 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Generate seam cell warping model\n",
    "exclude_seam_cells = ['QL', 'QR']\n",
    "seam_cell_strains = config['data']['seam_cells']\n",
    "smoothing_window = config['settings']['smoothing']['window_size'] \n",
    "\n",
    "# go through files and look for the strain names\n",
    "warp_strain_names = []\n",
    "for seam_strain_folder in seam_cell_strains:\n",
    "    cell_key_filepath_json = os.path.join(seam_strain_folder, 'cell_key.json')\n",
    "    \n",
    "    # if the json exists, use it\n",
    "    if os.path.isfile(cell_key_filepath_json):\n",
    "        with open(cell_key_filepath_json) as f:\n",
    "            strain_cell_key = json.load(f)\n",
    "            warp_strain_names.append(strain_cell_key['name'])\n",
    "\n",
    "# if not warp strain names, use a cached warping model\n",
    "seam_warp_model_folderpath = config['data']['seam_cells']\n",
    "print(\"Using the following strains for warping: {}\".format(\", \".join(warp_strain_names)))\n",
    "        \n",
    "# first combine all the necessary data for seam cells\n",
    "warp_model_by_cell = {}\n",
    "for strain in compiled_data_interp.keys():\n",
    "    for pos in compiled_data_interp[strain].keys():\n",
    "        \n",
    "        if pos not in warp_strain_names:\n",
    "            continue\n",
    "            \n",
    "        for cell in compiled_data_interp[strain][pos]['seam_cells'].keys():\n",
    "            if cell in exclude_seam_cells:\n",
    "                continue \n",
    "                \n",
    "            if cell not in warp_model_by_cell.keys():\n",
    "                warp_model_by_cell[cell] = []\n",
    "            \n",
    "            seam_cell_coordindates = np.array(\n",
    "                compiled_data_interp[strain][pos]['seam_cells'][cell]['coordinates'])\n",
    "            warp_model_by_cell[cell].append(\n",
    "                seam_cell_coordindates.tolist())\n",
    "            \n",
    "# average all coordinates by cell for warping model\n",
    "for cell in warp_model_by_cell.keys():\n",
    "    # average all the coordinates, step 5\n",
    "    cell_coords = np.array(warp_model_by_cell[cell])\n",
    "    warp_model_by_cell[cell] = np.average(warp_model_by_cell[cell], axis=0) # full coordinates\n",
    "    \n",
    "    # smooth, step 7\n",
    "    coordinates = warp_model_by_cell[cell]\n",
    "    for dim_idx in range(3):\n",
    "        coord_dim_data = coordinates[:, dim_idx].copy()\n",
    "        data_smoothed = moving_average(coord_dim_data, smoothing_window)\n",
    "        coordinates[:, dim_idx] = data_smoothed\n",
    "    \n",
    "    # convert to list\n",
    "    warp_model_by_cell[cell] = coordinates.tolist() # full coordinates\n",
    "    \n",
    "# convert to all cells for each timepoint\n",
    "total_len = config['settings']['interpolation']['total_min'] # in minutes\n",
    "new_timepoints = np.arange(0, total_len)\n",
    "seam_cells_on = config['settings']['interpolation']['seam_cells_on']\n",
    "sorted_seam_cells = sorted(warp_model_by_cell.keys()) # maintain some order\n",
    "warp_model_by_timepoint = []\n",
    "for timepoint in new_timepoints.astype(int):\n",
    "    timepoint_coords_by_cell = []\n",
    "    for seam_cell in sorted_seam_cells:\n",
    "        all_timepoint_coords = np.array(warp_model_by_cell[seam_cell])\n",
    "        seam_cell_coords = all_timepoint_coords[timepoint, :].tolist()\n",
    "        timepoint_coords_by_cell.append(seam_cell_coords)\n",
    "        \n",
    "    warp_model_by_timepoint.append(timepoint_coords_by_cell)\n",
    "\n",
    "print('Step 4 Completed.')\n",
    "warping_model = warp_model_by_timepoint # rename\n",
    "# save information as intermediate step in workspace\n",
    "output_json = {}\n",
    "output_json['seam_cells'] = sorted_seam_cells\n",
    "output_json['coordinates'] = warping_model\n",
    "warping_model = output_json\n",
    "\n",
    "compiled_json = json.dumps(output_json, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '4_seam_cell_warping_model.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 4 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warping OD1599_NU, Position OD1599_NU_1206_Pos2\n",
      "Warping OD1599_NU, Position OD1599_NU_1127_Pos3\n",
      "Warping OD1599_NU, Position OD1599_NU_1126_Pos0\n",
      "Warping DCR6485_RPM1_NU, Position ï»¿DCR6485NU_011419_Pos0\n",
      "Warping DCR6485_RPM1_NU, Position ï»¿DCR6485NU_011419_Pos4\n",
      "Warping DCR6485_RPM1_NU, Position ï»¿DCR6485NU_021020_Pos2\n",
      "Step 5 Completed.\n",
      "Step 5 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Warp strains to warping model\n",
    "# we need to do this first because seam cell information\n",
    "timepoints = np.arange(0, total_len).astype(int).tolist()\n",
    "compiled_data_warped = copy.deepcopy(compiled_data_interp)\n",
    "for strain in compiled_data_interp.keys():\n",
    "    for pos in compiled_data_interp[strain].keys():\n",
    "        print('Warping {}, Position {}'.format(strain, pos))\n",
    "        for timepoint in timepoints:\n",
    "            # get warp from/to model at timepoint (the seam cells)\n",
    "            pos_seam_cells = compiled_data_interp[strain][pos]['seam_cells']\n",
    "            warp_to_seam_cell_names = warping_model['seam_cells']\n",
    "            warp_to_seam_cell_coords = warping_model['coordinates']\n",
    "            warp_to = warp_to_seam_cell_coords[timepoint]\n",
    "            warp_from = [] # obtains the correct order since it's in a dict\n",
    "            for warp_to_seam_cell_name in warp_to_seam_cell_names:\n",
    "                if warp_to_seam_cell_name in pos_seam_cells.keys():\n",
    "                    all_time_coords = pos_seam_cells[warp_to_seam_cell_name]['coordinates']\n",
    "                    warp_from_timepoint_coord = all_time_coords[timepoint]\n",
    "                    warp_from.append(warp_from_timepoint_coord)\n",
    "            # print('from', np.array(warp_from))\n",
    "            # print('to', np.array(warp_to))\n",
    "            \n",
    "            # warp each cell at timepoint, including the seam cells themselves\n",
    "            for cell_type in ['seam_cells', 'annotations']:\n",
    "                # get cell order for warping\n",
    "                sorted_cell_names = compiled_data_interp[strain][pos][cell_type].keys()\n",
    "                ordered_coord_list = []\n",
    "                for cell_name in sorted_cell_names:\n",
    "                    old_coords = [compiled_data_interp[strain][pos][cell_type][cell_name]['coordinates'][timepoint]]\n",
    "                    old_coords = np.array(old_coords).flatten().tolist() # ensure correct data shape\n",
    "                    ordered_coord_list.append(old_coords)\n",
    "                \n",
    "                # warping\n",
    "                new_coords = thin_plate_spline_warp(warp_from, warp_to, ordered_coord_list)\n",
    "                new_coords = new_coords.tolist()\n",
    "                \n",
    "                # assigning\n",
    "                for cell_idx, cell_name in enumerate(sorted_cell_names):\n",
    "                    # if the old coordinates used to be 0 (ignored), then keep it that way\n",
    "                    old_coords = compiled_data_interp[strain][pos][cell_type][cell_name]['coordinates'][timepoint]\n",
    "                    \n",
    "                    if old_coords == [0, 0, 0]:\n",
    "                        assign_coords = old_coords\n",
    "                    else:\n",
    "                        assign_coords = new_coords[cell_idx]   \n",
    "                    compiled_data_warped[strain][pos][cell_type][cell_name]['coordinates'][timepoint] = \\\n",
    "                        assign_coords\n",
    "    \n",
    "print('Step 5 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "output_json = compiled_data_warped\n",
    "compiled_json = json.dumps(output_json, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '5_compiled_data_warped.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 5 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 Completed.\n",
      "Step 6 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Reformat data to average points\n",
    "step_data = copy.deepcopy(compiled_data_warped)\n",
    "data_by_cell = {'seam_cells':{}, 'annotations':{}}\n",
    "for strain in step_data.keys():\n",
    "    for pos in step_data[strain].keys():\n",
    "        for cell_type in ['seam_cells', 'annotations']:\n",
    "            for cell in step_data[strain][pos][cell_type].keys():\n",
    "                if cell not in data_by_cell.keys():\n",
    "                    data_by_cell[cell_type][cell] = []\n",
    "\n",
    "                cell_coordindates = np.array(\n",
    "                    step_data[strain][pos][cell_type][cell]['coordinates'])\n",
    "                data_by_cell[cell_type][cell].append(\n",
    "                    cell_coordindates.tolist())\n",
    "            \n",
    "# average all coordinates by cell\n",
    "for cell_type in ['seam_cells', 'annotations']:\n",
    "    # seam cells should just result in the average warping model\n",
    "    for cell in data_by_cell[cell_type].keys():\n",
    "        # average all the coordinates\n",
    "        cell_coords = np.array(data_by_cell[cell_type][cell])\n",
    "        data_by_cell[cell_type][cell] = np.average(data_by_cell[cell_type][cell], axis=0).tolist()\n",
    "\n",
    "print('Step 6 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "output_json = data_by_cell\n",
    "compiled_json = json.dumps(output_json, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '6_cell_coordinates_by_timepoint.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 6 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 Completed.\n",
      "Step 7 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Perform spatial moving average for each cell\n",
    "spatial_averaged = copy.deepcopy(data_by_cell)\n",
    "smoothing_window = config['settings']['smoothing']['window_size']\n",
    "\n",
    "# average all coordinates by cell\n",
    "for cell_type in ['seam_cells', 'annotations']:\n",
    "    # seam cells should just result in the average warping model\n",
    "    for cell in spatial_averaged[cell_type].keys():\n",
    "        # spatial average all the coordinates by time\n",
    "        coordinates = np.array(spatial_averaged[cell_type][cell])\n",
    "        # go through each axis\n",
    "        for dim_idx in range(3):\n",
    "            coord_dim_data = coordinates[:, dim_idx].copy()\n",
    "            data_smoothed = moving_average(coord_dim_data, smoothing_window)\n",
    "            coordinates[:, dim_idx] = data_smoothed\n",
    "\n",
    "        spatial_averaged[cell_type][cell] = coordinates.tolist()\n",
    "\n",
    "print('Step 7 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "output_json = spatial_averaged\n",
    "compiled_json = json.dumps(output_json, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '7_cell_coordinates_by_timepoint_smoothed.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 7 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 Completed.\n",
      "Output location: Y:\\RyanC\\model_building_code\\workspace\\2020_08_16-OD1599_NU+NerveRing\\output\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Convert to csv in MIPAV format\n",
    "\n",
    "# create timepoints array again just to be safe\n",
    "total_len = config['settings']['interpolation']['total_min'] # in minutes\n",
    "timepoints = np.arange(0, total_len)\n",
    "\n",
    "# generate place to put all the files\n",
    "output_folder = os.path.join(workspace_folderpath, 'output')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "                \n",
    "\n",
    "for cell_type in ['seam_cells', 'annotations']:\n",
    "    cells = spatial_averaged[cell_type]\n",
    "\n",
    "    for cell_name in cells.keys():\n",
    "        filename_csv = cell_name +'.csv'\n",
    "        filepath_csv = os.path.join(output_folder, filename_csv)\n",
    "\n",
    "        coordinates = spatial_averaged[cell_type][cell_name]\n",
    "        new_table = pd.DataFrame(coordinates) * 0.1625\n",
    "        new_table.columns = ['x', 'y', 'z']\n",
    "        new_table.insert(0, \"timepoints\", timepoints, True)\n",
    "        # colors\n",
    "        placeholder_color = np.ones((total_len,)) * 255\n",
    "        new_table.insert(4, \"R\", placeholder_color, True)\n",
    "        new_table.insert(5, \"G\", placeholder_color, True)\n",
    "        new_table.insert(6, \"B\", placeholder_color, True)\n",
    "        new_table.insert(7, \"A\", np.zeros((total_len,)), True)\n",
    "\n",
    "        new_table.to_csv(filepath_csv, index=False, header=False)\n",
    "        \n",
    "full_output_folderpath = os.path.join(os.getcwd(), output_folder)\n",
    "print('Step 8 Completed.')\n",
    "print('Output location: {}'.format(full_output_folderpath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
