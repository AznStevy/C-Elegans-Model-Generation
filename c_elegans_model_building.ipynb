{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Reset variables\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"RW10752_NU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     1,
     13,
     30,
     64
    ]
   },
   "outputs": [],
   "source": [
    "# Define some helper functions\n",
    "def create_workspace_folders():\n",
    "    date = datetime.datetime.now()\n",
    "    workspace = \"workspace\\{}-{}\".format(date.strftime(\"%Y_%m_%d\"), name)\n",
    "\n",
    "    create_folders = ['workspace', workspace]\n",
    "    # create workspace\n",
    "    for folder in create_folders:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "    \n",
    "    return workspace\n",
    "\n",
    "def moving_average(a, window_size):\n",
    "    new_a = []\n",
    "    half_window = round(window_size/2)\n",
    "    for idx, val in enumerate(a):\n",
    "        if val == 0:\n",
    "            new_a.append(0)\n",
    "            continue\n",
    "            \n",
    "        start_idx = max(idx-half_window, 0)\n",
    "        end_idx = min(idx+half_window+1, a.shape[0])\n",
    "        a_segment = a[start_idx:end_idx]\n",
    "        \n",
    "        a_seg_mean = np.nanmean(np.where(a_segment!=0, a_segment, np.nan)) # nonzero mean\n",
    "        new_a.append(a_seg_mean)\n",
    "        \n",
    "    return new_a\n",
    "\n",
    "def hampel_filter(input_series, window=5, n_sigmas=1):\n",
    "    # ensure the data is flattened\n",
    "    input_series = np.array(input_series).flatten()\n",
    "    \n",
    "    # returns filtered timepoints and coordinates along\n",
    "    n = len(input_series)\n",
    "    k = 1.4826 # scale factor for Gaussian distribution\n",
    "    \n",
    "    outliers_idxs = []\n",
    "    outliers_filtered = []\n",
    "    extended_series = np.pad(input_series, (window, window), 'reflect')\n",
    "    filtered_series = extended_series.copy()\n",
    "    n_ex = len(extended_series)\n",
    "    for i in range(window_size, n_ex - window_size):\n",
    "        x0 = np.median(\n",
    "            extended_series[(i - window_size):(i + window_size)])\n",
    "        S0 = k * np.median(np.abs(\n",
    "            extended_series[(i - window_size):(i + window_size)] - x0))\n",
    "        if (np.abs(extended_series[i] - x0) > n_sigmas * S0):\n",
    "            filtered_series[i] = x0 # replaces outlier with median\n",
    "            outliers_idxs.append(i-window) # logs outlier index\n",
    "            outliers_filtered.append(x0) # the value that replaces outlier\n",
    "    \n",
    "    filtered_series = filtered_series[window:len(filtered_series)-window]\n",
    "    \n",
    "    return filtered_series, outliers_idxs, outliers_filtered\n",
    "\n",
    "def write_error(workspace_folderpath, error, write_type=\"w\"):    \n",
    "    error_path = os.path.join(workspace_folderpath, 'errors.txt')\n",
    "    \n",
    "    with open(error_path, write_type) as f:\n",
    "        f.write(error)\n",
    "\n",
    "# try thin plate spline warping\n",
    "def thin_plate_spline_warp(unwarped_pts, ctrl_pts, obj_to_warp):\n",
    "\n",
    "    # convert everything to np array\n",
    "    unwarped_pts = np.array(unwarped_pts)\n",
    "    ctrl_pts = np.array(ctrl_pts)\n",
    "    obj_to_warp = np.array(obj_to_warp)\n",
    "\n",
    "    num_points = unwarped_pts.shape[0]\n",
    "    K = np.zeros((num_points, num_points))\n",
    "    for rr in np.arange(num_points):\n",
    "        for cc in np.arange(num_points):\n",
    "            K[rr,cc] = np.sum(np.subtract(unwarped_pts[rr,:], unwarped_pts[cc,:])**2) #R**2 \n",
    "            K[cc,rr] = K[rr,cc]\n",
    "\n",
    "    #calculate kernel function R\n",
    "    K = np.maximum(K, 1e-320) \n",
    "    #K = K.* log(sqrt(K))\n",
    "    K = np.sqrt(K) #\n",
    "    # Calculate P matrix\n",
    "    P = np.hstack((np.ones((num_points, 1)), unwarped_pts)) #nX4 for 3D\n",
    "    # Calculate L matrix\n",
    "    L_top = np.hstack((K, P))\n",
    "    L_bot = np.hstack((P.T, np.zeros((4,4))))\n",
    "    L = np.vstack((L_top, L_bot))\n",
    "\n",
    "    param = np.matmul(np.linalg.pinv(L), np.vstack((ctrl_pts, np.zeros((4,3)))))\n",
    "    # Calculate new coordinates (x',y',z') for each points \n",
    "    num_points_obj = obj_to_warp.shape[0]\n",
    "\n",
    "    K = np.zeros((num_points_obj, num_points))\n",
    "    gx = obj_to_warp[:,0]\n",
    "    gy = obj_to_warp[:,1]\n",
    "    gz = obj_to_warp[:,2]\n",
    "\n",
    "    for nn in np.arange(num_points):\n",
    "        K[:,nn] = \\\n",
    "        np.square(np.subtract(gx, unwarped_pts[nn,0])) + \\\n",
    "        np.square(np.subtract(gy, unwarped_pts[nn,1])) + \\\n",
    "        np.square(np.subtract(gz, unwarped_pts[nn,2])) # R**2\n",
    " \n",
    "    K = np.maximum(K, 1e-320) \n",
    "    K = np.sqrt(K) #|R| for 3D\n",
    "    gx = np.vstack(obj_to_warp[:,0])\n",
    "    gy = np.vstack(obj_to_warp[:,1])\n",
    "    gz = np.vstack(obj_to_warp[:,2])\n",
    "    P = np.hstack((np.ones((num_points_obj,1)), gx, gy, gz))\n",
    "    L = np.hstack((K, P))\n",
    "    object_warped = np.matmul(L, param)\n",
    "    object_warped[:,0] = np.round(object_warped[:,0]*10**3)*10**-3\n",
    "    object_warped[:,1] = np.round(object_warped[:,1]*10**3)*10**-3\n",
    "    object_warped[:,2] = np.round(object_warped[:,2]*10**3)*10**-3\n",
    "\n",
    "    return object_warped\n",
    "\n",
    "def thin_plate_spline_warp_c_elegans(unwarped_pts, ctrl_pts, obj_to_warp):\n",
    "    \"\"\"Warps, then shifts xy based on bounding box. The goal is to have everything on the xz plane/y=0\n",
    "    before we start warping.\n",
    "    \"\"\"\n",
    "    # bring everything up to the unwarped plane in x and y. z is length-wise\n",
    "    unwarped_half_bound = np.mean(unwarped_pts[:, 1])\n",
    "    x_shift = unwarped_half_bound - (np.mean(ctrl_pts[:, 0]))\n",
    "    y_shift = unwarped_half_bound - (np.mean(ctrl_pts[:, 1]))\n",
    "    ctrl_pts[:, 0] += x_shift\n",
    "    ctrl_pts[:, 1] += y_shift\n",
    "    \n",
    "    # perform warp\n",
    "    object_warped = thin_plate_spline_warp(unwarped_pts, ctrl_pts, obj_to_warp)\n",
    "\n",
    "    # subtract bounding box in x and y\n",
    "    object_warped[:, 0:2] -= unwarped_half_bound # match for points to warp. seam cells and annotations\n",
    "    \n",
    "    for coord_idx, coord in enumerate(unwarped_pts):\n",
    "        # if at the origin, then keep y at origin too\n",
    "        if coord[0] == 0 and coord[2] == 0:\n",
    "            object_warped[coord_idx, 1] = 0\n",
    "            \n",
    "    return object_warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace folderpath: workspace\\2020_09_09-RW10752_NU\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "# Create name + workspace folders\n",
    "workspace_folderpath = create_workspace_folders()\n",
    "print('Workspace folderpath: {}'.format(workspace_folderpath))\n",
    "\n",
    "# Load necessary file structures via config file\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Clear error report\n",
    "write_error(workspace_folderpath, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     1,
     168,
     191
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\RW10752_NU\\Untwisting\\031219_Pos2\n",
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\RW10752_NU\\Untwisting\\031219_Pos1\n",
      "CURRENTLY LOADING: Y:\\RyanC\\Cell Tracking Project\\RW10752_NU\\Untwisting\\022519_Pos0\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import all the necessary data\n",
    "def parse_mipav_data(config, cell_key, strain_folderpath):\n",
    "    # define output variables\n",
    "    valid_seam_cells = set(config['settings']['interpolation']['seam_cells_on'].keys())\n",
    "    seam_cell_output = {}\n",
    "    annotation_output = {} # structured cell_name: pandas dataframe (timepoint, x, y, z)\n",
    "    errors = [] # note potential errors in volumes\n",
    "    \n",
    "    # determine default or options specific to strain\n",
    "    folderpaths = config['settings']['folderpaths']\n",
    "    for folder_data in folderpaths.keys():\n",
    "        try:\n",
    "            folderpaths[folder_data] = cell_key['folderpaths'][folder_data]\n",
    "            print('Using specific {} folder structure'.format(folder_data))\n",
    "        except:\n",
    "            # print('Using default {} folder structure'.format(folder_data))\n",
    "            pass\n",
    "    \n",
    "    # find the numbers of the folders\n",
    "    start_vol = int(cell_key['start'])\n",
    "    end_vol = int(cell_key['end'])\n",
    "    all_folderpaths = folderpaths.copy()\n",
    "    for vol_idx, vol_num in enumerate(range(start_vol, end_vol+1)):\n",
    "        # ignore outliers\n",
    "        \"\"\"\n",
    "        if vol_num in cell_key['outliers']:\n",
    "            continue\"\"\"\n",
    "        \n",
    "        # define the specific volume number folder\n",
    "        vol_folder = all_folderpaths['data_folderpath'].replace('#', str(vol_num))\n",
    "        vol_folderpath = os.path.join(\n",
    "            strain_folderpath, all_folderpaths['side'], vol_folder)\n",
    "        \n",
    "        # check if the folder exists\n",
    "        if not os.path.isdir(vol_folderpath):\n",
    "            continue\n",
    "        \n",
    "        # define individual, full filepaths from drive\n",
    "        all_filepaths_folderpaths = {}\n",
    "        for full_folderpaths_key in all_folderpaths.keys():\n",
    "            if full_folderpaths_key != \"side\" and \\\n",
    "                full_folderpaths_key != \"data_folderpath\":\n",
    "                all_filepaths_folderpaths[full_folderpaths_key] = os.path.join(\n",
    "                    vol_folderpath, all_folderpaths[full_folderpaths_key])\n",
    "                # print(all_filepaths_folderpaths[full_folderpaths_key])\n",
    "\n",
    "                # check to see if exists\n",
    "                if not os.path.isfile(all_filepaths_folderpaths[full_folderpaths_key]):\n",
    "                    error_msg = \"FILE DOES NOT EXIST: {}\".format(\n",
    "                        all_filepaths_folderpaths[full_folderpaths_key])\n",
    "                    errors.append(error_msg)\n",
    "                    print(error_msg)\n",
    "        \n",
    "        ## get individual data by cell in pandas format\n",
    "        try:\n",
    "            # import twisted seam cells           \n",
    "            tw_seam_cells_fp = all_filepaths_folderpaths['twisted_seam_cells']\n",
    "            tw_seam_cells = pd.read_csv(tw_seam_cells_fp)            \n",
    "            # import twisted annotations\n",
    "            tw_annotations_fp = all_filepaths_folderpaths['twisted_annotations']\n",
    "            tw_annotations = pd.read_csv(tw_annotations_fp)\n",
    "            # import straighted seam cells\n",
    "            st_seam_cells_fp = all_filepaths_folderpaths['straightened_seam_cells']\n",
    "            st_seam_cells = pd.read_csv(st_seam_cells_fp)\n",
    "            # import straightened annotations\n",
    "            st_annotations_fp = all_filepaths_folderpaths['straightened_annotations']\n",
    "            st_annotations = pd.read_csv(st_annotations_fp)\n",
    "        except:\n",
    "            continue # file errors should already be logged.\n",
    "            \n",
    "        # ERROR CHECKING: Check for file content ----------------------\n",
    "        if st_seam_cells.size == 0: # check if empty\n",
    "            error_msg = \"DATA ERROR: Empty seam cell file at {}\".format(st_seam_cells_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        if st_annotations.size == 0: # check if empty\n",
    "            error_msg = \"DATA ERROR: Empty annotation file at {}\".format(st_annotations_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        data_file_list = [ # use to check values of files\n",
    "            (tw_seam_cells, tw_seam_cells_fp), (tw_annotations, tw_annotations_fp),\n",
    "            (st_seam_cells, st_seam_cells_fp), (st_annotations, st_annotations_fp)\n",
    "        ]\n",
    "        for data, fp in data_file_list:\n",
    "            coordinates = data[['x_voxels', 'y_voxels', 'z_voxels']]\n",
    "            cell_ids = data['name'].values.tolist()\n",
    "            for cell_idx, cell_coord in coordinates.iterrows():\n",
    "                if not cell_coord.equals(cell_coord.astype(float)):\n",
    "                    error_msg = \"DATA ERROR: Non-float found for cell {} in {}\".format(\n",
    "                        cell_ids[cell_idx], fp)\n",
    "                    errors.append(error_msg)\n",
    "                    print(error_msg)            \n",
    "        seam_cells_mismatch = list(set(st_seam_cells['name'].values.tolist()) - \n",
    "                                   set(tw_seam_cells['name'].values.tolist()))\n",
    "        annotations_mismatch = list(set(st_annotations['name'].values.tolist()) - \n",
    "                                   set(tw_annotations['name'].values.tolist())) \n",
    "        if seam_cells_mismatch:\n",
    "            error_msg = \"DATA ERROR: Mis-match between seam cells ({}) found in {} and {}\".format(\n",
    "                \", \".join(seam_cells_mismatch), st_seam_cells_fp, tw_seam_cells_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        if annotations_mismatch:\n",
    "            error_msg = \"DATA ERROR: Mis-match between annotated cells ({}) found in {} and {}\".format(\n",
    "                \", \".join(annotations_mismatch), st_annotations_fp, tw_annotations_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        # check if extra seam cells exist/mismatch with settings\n",
    "        extra_seam_cells = list(set(st_seam_cells['name'].values.tolist()) - valid_seam_cells)\n",
    "        if extra_seam_cells: # check if there are extra seam cells\n",
    "            # save old one as _bak.csv, save new as straightened_seamcells.csv\n",
    "            # st_seam_cells.to_csv(st_seam_cells_fp.replace('.csv', '_bak.csv'), index=False)\n",
    "            for extra_seam_cell in extra_seam_cells:\n",
    "                st_seam_cells = st_seam_cells[st_seam_cells['name'] != extra_seam_cell]\n",
    "            # st_seam_cells.to_csv(st_seam_cells_fp, index=False)\n",
    "            error_msg = \"DATA WARNING: Ignoring extra seam cells ({}) found in {}\".format(\n",
    "                \", \".join(extra_seam_cells), st_seam_cells_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "        # END FILE ERROR CHECK -----------------------------------------------------------\n",
    "\n",
    "        # SEAM CELLS: combine into a single data structure and catch data errors\n",
    "        # go through each row of the file and log the appropriate information\n",
    "        for idx, seam_row in st_seam_cells.iterrows():\n",
    "            seam_cell_name = seam_row['name'].upper() # seam cells are upper case? just stay consistent\n",
    "            if seam_cell_name not in seam_cell_output.keys():\n",
    "                seam_cell_output[seam_cell_name] = {}\n",
    "                seam_cell_output[seam_cell_name]['timepoints'] = []\n",
    "                seam_cell_output[seam_cell_name]['coordinates'] = []\n",
    "\n",
    "            # append to appropriate seam cell\n",
    "            if vol_num not in seam_cell_output[seam_cell_name]['timepoints']:\n",
    "                seam_cell_coords = seam_row[['x_voxels','y_voxels','z_voxels']].values.flatten().tolist()\n",
    "                seam_cell_output[seam_cell_name]['coordinates'].append(seam_cell_coords)\n",
    "                seam_cell_output[seam_cell_name]['timepoints'].append(vol_num)\n",
    "\n",
    "        # ANNOTATIONS: combine into a single data structure and catch data errors\n",
    "        try:\n",
    "            for cell_id in cell_key['mapping'].keys():\n",
    "                cell_name = cell_key['mapping'][cell_id].lower()\n",
    "                if cell_name not in annotation_output.keys():\n",
    "                    annotation_output[cell_name] = {}\n",
    "                    annotation_output[cell_name]['timepoints'] = []\n",
    "                    annotation_output[cell_name]['coordinates'] = []\n",
    "\n",
    "                # check if the cell name exists in volume\n",
    "                # print(vol_folderpath, st_annotations['name'], cell_id)\n",
    "                cell_row = st_annotations.loc[st_annotations['name'] == cell_id]\n",
    "                if not cell_row.empty:\n",
    "                    # try to catch a few errors\n",
    "                    if cell_row.shape[0] > 1:\n",
    "                        error_msg = \"DATA ERROR: Ignoring identical cell IDs ({}) for cell {} in {}\".format(\n",
    "                            cell_id, cell_name, st_annotations_fp)\n",
    "                        errors.append(error_msg)\n",
    "                        print(error_msg)\n",
    "                        continue\n",
    "                    \n",
    "                    # if there is nothing wrong, then proceed\n",
    "                    if vol_num not in annotation_output[cell_name]['timepoints']:\n",
    "                        cell_coords = cell_row[['x_voxels','y_voxels','z_voxels']].values.flatten().tolist()\n",
    "                        annotation_output[cell_name]['coordinates'].append(cell_coords)\n",
    "                        annotation_output[cell_name]['timepoints'].append(vol_num)\n",
    "        except:\n",
    "            error_msg = \"DATA ERROR: Failure to read file {}\".format(st_annotations_fp)\n",
    "            errors.append(error_msg)\n",
    "            print(error_msg)\n",
    "                \n",
    "    return seam_cell_output, annotation_output, errors\n",
    "\n",
    "def convert_cell_key_csv2json(csv_filepath):\n",
    "    cell_key_json = {}\n",
    "    # load in csv file\n",
    "    cell_key = pd.read_csv(csv_filepath, header=None, engine='python')\n",
    "    \n",
    "    ## get necessary information\n",
    "    cell_key_json['name'] = str(cell_key.iloc[0,0])\n",
    "    cell_key_json['start'] = int(cell_key.iloc[1,0])\n",
    "    cell_key_json['end'] = int(cell_key.iloc[1,1])\n",
    "    try:\n",
    "        cell_key_json['outliers'] = cell_key.iloc[2].astype(int).values.tolist()\n",
    "    except:\n",
    "        cell_key_json['outliers'] = [] # if there are none\n",
    "    \n",
    "    # grab mapping\n",
    "    cell_key_json['mapping'] = {}\n",
    "    for row_idx in range(3, cell_key.shape[0]): # mapping starts row idx 3\n",
    "        cell_id = str(cell_key.iloc[row_idx,0])\n",
    "        cell_name = str(cell_key.iloc[row_idx,1])\n",
    "        cell_key_json['mapping'][cell_id] = cell_name\n",
    "\n",
    "    return cell_key_json\n",
    "    \n",
    "def get_cell_key(pos_folderpath):\n",
    "    # return the cell key in dict/json format and convert any existing \n",
    "    # cell key csv into json if the json version doesn't exist.\n",
    "    cell_key_filepath_csv = os.path.join(pos_folderpath, 'CellKey.csv')\n",
    "    cell_key_filepath_json = os.path.join(pos_folderpath, 'cell_key.json')\n",
    "    \n",
    "    # if the json exists, use it\n",
    "    if os.path.isfile(cell_key_filepath_json):\n",
    "        with open(cell_key_filepath_json) as f:\n",
    "            return json.load(f)\n",
    "    elif os.path.isfile(cell_key_filepath_csv):\n",
    "        cell_key_json = convert_cell_key_csv2json(cell_key_filepath_csv)\n",
    "        cell_key_json_dump = json.dumps(cell_key_json, sort_keys=True, indent=4)\n",
    "        with open(cell_key_filepath_json, \"w\") as f: \n",
    "            f.write(cell_key_json_dump)\n",
    "        return cell_key_json\n",
    "    else:\n",
    "        cell_key_error_msg = 'Missing cell key: {}'.format(cell_key_filepath_json)\n",
    "        print(cell_key_error_msg)\n",
    "        return None\n",
    "                \n",
    "strain_info = config['data']['strains']\n",
    "compiled_data = {}\n",
    "for strain in strain_info:\n",
    "    if strain['include']:\n",
    "        if strain['name'] not in compiled_data.keys():\n",
    "            compiled_data[strain['name']] = {}\n",
    "                \n",
    "        for pos_folderpath in strain['folderpaths']:\n",
    "            print('CURRENTLY LOADING: {}'.format(pos_folderpath))\n",
    "            # find and load cell key\n",
    "            cell_key = get_cell_key(pos_folderpath)\n",
    "            if not cell_key:\n",
    "                continue\n",
    "                \n",
    "            # get data\n",
    "            seam_cells, annotations, strain_errors = parse_mipav_data(\n",
    "                config, cell_key, pos_folderpath)\n",
    "            \n",
    "            # combine data\n",
    "            pos_name = cell_key['name']\n",
    "            compiled_data[strain['name']][pos_name] = {}\n",
    "            compiled_data[strain['name']][pos_name]['cell_key'] = cell_key\n",
    "            compiled_data[strain['name']][pos_name]['seam_cells'] = seam_cells\n",
    "            compiled_data[strain['name']][pos_name]['annotations'] = annotations\n",
    "            compiled_data[strain['name']][pos_name]['errors'] = strain_errors\n",
    "            \n",
    "            # write necessary errors to file\n",
    "            if strain_errors:\n",
    "                write_error(workspace_folderpath, '\\n'.join(strain_errors) + '\\n', 'a')\n",
    "\n",
    "# save information as intermediate step in workspace\n",
    "compiled_json = json.dumps(compiled_data, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '1_compiled_data.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 Completed.\n",
      "Step 2 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Check for outliers, uses parameters defined in config.json\n",
    "\n",
    "window_size = config['settings']['outlier_removal']['window_size']\n",
    "n_stdev = config['settings']['outlier_removal']['n_stdev']\n",
    "compiled_data_no_outliers = copy.deepcopy(compiled_data)\n",
    "for strain in compiled_data.keys():\n",
    "    for pos in compiled_data[strain].keys():\n",
    "\n",
    "        # determine outliers for both seam cells and annotations\n",
    "        for cells_type in ['seam_cells', 'annotations']:\n",
    "            for cell in compiled_data[strain][pos][cells_type].keys():\n",
    "                coordinates = np.array(compiled_data[strain][pos][cells_type][cell]['coordinates'])\n",
    "\n",
    "                # separate by x,y,z (index 0-2) and filter\n",
    "                outlier_idxs = {} # set automatically removes repeats\n",
    "                for dim_idx in range(3):\n",
    "                    coord_dim_data = copy.deepcopy(coordinates[:, dim_idx])\n",
    "                    data_filtered, outlier_idx, outlier_val = hampel_filter(coord_dim_data, \n",
    "                            n_sigmas=n_stdev, window=window_size)\n",
    "                    coordinates[:, dim_idx] = data_filtered\n",
    "\n",
    "                # replace time series outliers using median\n",
    "                compiled_data_no_outliers[strain][pos][cells_type][cell]['coordinates'] = coordinates.tolist()\n",
    "\n",
    "print('Step 2 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "compiled_json = json.dumps(compiled_data_no_outliers, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '2_compiled_data_no_outliers.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 2 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 Completed.\n",
      "Step 3 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Interpolate each to appropriate time scale\n",
    "compiled_data_interp = copy.deepcopy(compiled_data_no_outliers)\n",
    "seam_cells_on = config['settings']['interpolation']['seam_cells_on'] # in minutes\n",
    "total_len = config['settings']['interpolation']['total_min'] # in minutes\n",
    "interp_method = config['settings']['interpolation']['method']\n",
    "min_timepoints_required = config['settings']['interpolation']['min_timepoints_required']\n",
    "new_timepoints = np.linspace(0, total_len)\n",
    "\n",
    "for strain in compiled_data_no_outliers.keys():\n",
    "    for pos in compiled_data_no_outliers[strain].keys():\n",
    "        \n",
    "        cell_key = compiled_data_no_outliers[strain][pos]['cell_key']\n",
    "        # interpolate for both seam cells and annotations\n",
    "        for cells_type in ['seam_cells', 'annotations']:\n",
    "            for cell in compiled_data_no_outliers[strain][pos][cells_type].keys():\n",
    "                timepoints = np.array(compiled_data_no_outliers[strain][pos][cells_type][cell]['timepoints'])\n",
    "                coordinates = np.array(compiled_data_no_outliers[strain][pos][cells_type][cell]['coordinates'])\n",
    "                \n",
    "                # CHECKS if there are a sufficient number of timepoints. if not, raise error.\n",
    "                if timepoints.size < min_timepoints_required:\n",
    "                    error_msg = \"DATA ERROR: Insufficient timepoints in {}/{} for cell {} where only {} (volumes {}), less than the required {}, exist.\".format(\n",
    "                        strain, pos, cell, str(len(timepoints)), \n",
    "                        \",\".join(timepoints.astype(str).tolist()), str(min_timepoints_required))\n",
    "                    compiled_data_interp[strain][pos]['errors'].append(error_msg)\n",
    "                    print(error_msg)\n",
    "                    write_error(workspace_folderpath, error_msg + '\\n', 'a')\n",
    "                    continue\n",
    "                \n",
    "                # handle seam cells and annotations differently because \n",
    "                # some seam cells appear after twitching begins\n",
    "                if cells_type == 'seam_cells':\n",
    "                    # determine if strain starts after designated seam cell on\n",
    "                    target_percent = seam_cells_on[cell]\n",
    "                    # target_starting_idx = int(round(target_percent * total_len)) # after scaling\n",
    "                    \n",
    "                    # find the closest volume to the target\n",
    "                    vol_len = cell_key['end'] - cell_key['start']\n",
    "                    og_target = round(vol_len * target_percent + cell_key['start'])\n",
    "                    og_target_idx = np.argmin(np.abs(timepoints - og_target)).astype(int) # in original time\n",
    "                    seam_cell_on_vol = timepoints[og_target_idx]\n",
    "                    seam_cell_on_percent = (seam_cell_on_vol - cell_key['start'])/vol_len # calculate what the actual percentage is\n",
    "                    \n",
    "                    # determine what to do if greater than or less than target percentage\n",
    "                    og_starting_idx = og_target_idx\n",
    "                    if seam_cell_on_percent > target_percent: # starts after expected\n",
    "                        starting_idx = int(round(seam_cell_on_percent * total_len))\n",
    "                    elif seam_cell_on_percent <= target_percent: # starts before expected\n",
    "                        starting_idx = int(round(target_percent * total_len))\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    if cell == \"QL\":\n",
    "                        print(cell, timepoints)\n",
    "                        print(seam_cells_on[cell], seam_cell_on_percent, og_target, \n",
    "                              cell_key['start'], og_starting_idx, starting_idx)\"\"\"\n",
    "                    \n",
    "                elif cells_type == 'annotations':\n",
    "                    starting_idx = 0\n",
    "                    og_starting_idx = 0\n",
    "                \n",
    "                new_coordinates = np.zeros((total_len, 3))\n",
    "                new_timepoints = np.arange(total_len)\n",
    "\n",
    "                # interpolate one dimension at a time\n",
    "                for dim_idx in range(3):\n",
    "                    # crop the original data to the appropriate length, e.g. Q: top 25% of volumes\n",
    "                    cell_timepoints = timepoints[og_starting_idx:].copy()\n",
    "                    coord_dim_data = coordinates[og_starting_idx:, dim_idx].copy()\n",
    "\n",
    "                    # rescale time points to range, 0 to end, rather than e.g. 8 to 54\n",
    "                    cell_sp_rescaled = cell_timepoints - min(cell_timepoints)\n",
    "                    cell_sp_rescaled = cell_sp_rescaled/max(cell_sp_rescaled) * (total_len - starting_idx)\n",
    "                \n",
    "                    interp = interpolate.interp1d(cell_sp_rescaled, coord_dim_data,\n",
    "                                     kind=interp_method) # get interp as if from 0, but shift below\n",
    "                    \n",
    "                    # new time scale for specific length (might be part of total)  \n",
    "                    cell_sp_timepoints = np.arange(total_len - starting_idx)\n",
    "                    interped_coords = interp(cell_sp_timepoints)\n",
    "                    new_coordinates[starting_idx:, dim_idx] = interped_coords # shifted\n",
    "\n",
    "                # replace time series outliers using median\n",
    "                compiled_data_interp[strain][pos][cells_type][cell]['coordinates'] = new_coordinates.tolist()\n",
    "                compiled_data_interp[strain][pos][cells_type][cell]['timepoints'] = new_timepoints.tolist()\n",
    "                \n",
    "# save information as intermediate step in workspace\n",
    "print('Step 3 Completed.')\n",
    "compiled_json = json.dumps(compiled_data_interp, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '3_compiled_data_interpolation.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 3 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     31
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following strains for warping: RW10752_0312_Pos2, RW10752_0312_Pos1, RW10752_0225_Pos0\n",
      "Step 4 Completed.\n",
      "Step 4 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Generate seam cell warping model\n",
    "exclude_seam_cells = ['QL', 'QR']\n",
    "seam_cell_strains = config['data']['seam_cells']\n",
    "smoothing_window = config['settings']['smoothing']['window_size']\n",
    "\n",
    "# check if it's a pre-generated warping model\n",
    "pre_generated_model = False\n",
    "if '.json' in seam_cell_strains[0]:\n",
    "    pre_generated_model = True\n",
    "    with open(seam_cell_strains[0]) as f:\n",
    "        output_json = json.load(f)\n",
    "\n",
    "    print(\"Using pre-generated warping model: {}\".format(seam_cell_strains[0]))\n",
    "else: # generate new warping model based on the strains given\n",
    "    # go through files and look for the strain names\n",
    "    warp_strain_names = []\n",
    "    for seam_strain_folder in seam_cell_strains:\n",
    "        cell_key_filepath_json = os.path.join(seam_strain_folder, 'cell_key.json')\n",
    "\n",
    "        # if the json exists, use it\n",
    "        if os.path.isfile(cell_key_filepath_json):\n",
    "            with open(cell_key_filepath_json) as f:\n",
    "                strain_cell_key = json.load(f)\n",
    "                warp_strain_names.append(strain_cell_key['name'])\n",
    "\n",
    "    # if not warp strain names, use a cached warping model\n",
    "    seam_warp_model_folderpath = config['data']['seam_cells']\n",
    "    print(\"Using the following strains for warping: {}\".format(\", \".join(warp_strain_names)))\n",
    "\n",
    "    # first combine all the necessary data for seam cells\n",
    "    warp_model_by_cell = {}\n",
    "    for strain in compiled_data_interp.keys():\n",
    "        for pos in compiled_data_interp[strain].keys():\n",
    "\n",
    "            if pos not in warp_strain_names:\n",
    "                continue\n",
    "\n",
    "            for cell in compiled_data_interp[strain][pos]['seam_cells'].keys():\n",
    "                if cell in exclude_seam_cells:\n",
    "                    continue \n",
    "\n",
    "                if cell not in warp_model_by_cell.keys():\n",
    "                    warp_model_by_cell[cell] = []\n",
    "\n",
    "                seam_cell_coordindates = np.array(\n",
    "                    compiled_data_interp[strain][pos]['seam_cells'][cell]['coordinates'])\n",
    "                warp_model_by_cell[cell].append(\n",
    "                    seam_cell_coordindates.tolist())\n",
    "\n",
    "    # average all coordinates by cell for warping model\n",
    "    for cell in warp_model_by_cell.keys():\n",
    "        # average all the coordinates, step 5\n",
    "        cell_coords = np.array(warp_model_by_cell[cell])\n",
    "        warp_model_by_cell[cell] = np.average(warp_model_by_cell[cell], axis=0) # full coordinates\n",
    "\n",
    "        # smooth, step 7\n",
    "        coordinates = warp_model_by_cell[cell]\n",
    "        for dim_idx in range(3):\n",
    "            coord_dim_data = coordinates[:, dim_idx].copy()\n",
    "            data_smoothed = moving_average(coord_dim_data, smoothing_window)\n",
    "            coordinates[:, dim_idx] = data_smoothed\n",
    "\n",
    "        # convert to list\n",
    "        warp_model_by_cell[cell] = coordinates.tolist() # full coordinates\n",
    "\n",
    "    # convert to all cells for each timepoint\n",
    "    total_len = config['settings']['interpolation']['total_min'] # in minutes\n",
    "    new_timepoints = np.arange(0, total_len)\n",
    "    seam_cells_on = config['settings']['interpolation']['seam_cells_on']\n",
    "    sorted_seam_cells = sorted(warp_model_by_cell.keys()) # maintain some order\n",
    "    warp_model_by_timepoint = []\n",
    "    for timepoint in new_timepoints.astype(int):\n",
    "        timepoint_coords_by_cell = []\n",
    "        for seam_cell in sorted_seam_cells:\n",
    "            all_timepoint_coords = np.array(warp_model_by_cell[seam_cell])\n",
    "            seam_cell_coords = all_timepoint_coords[timepoint, :].tolist()\n",
    "            timepoint_coords_by_cell.append(seam_cell_coords)\n",
    "\n",
    "        warp_model_by_timepoint.append(timepoint_coords_by_cell)\n",
    "\n",
    "    # create output structure\n",
    "    output_json = {}\n",
    "    output_json['seam_cells'] = sorted_seam_cells\n",
    "    output_json['coordinates'] = warp_model_by_timepoint\n",
    "\n",
    "print('Step 4 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "warping_model = output_json\n",
    "compiled_json = json.dumps(output_json, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '4_seam_cell_warping_model.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 4 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warping RW10752_NU, Position RW10752_0312_Pos2\n",
      "Warping RW10752_NU, Position RW10752_0312_Pos1\n",
      "Warping RW10752_NU, Position RW10752_0225_Pos0\n",
      "Step 5 Completed.\n",
      "Step 5 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Warp strains to warping model\n",
    "# we need to do this first because seam cell information\n",
    "timepoints = np.arange(0, total_len).astype(int).tolist()\n",
    "compiled_data_warped = copy.deepcopy(compiled_data_interp)\n",
    "for strain in compiled_data_interp.keys():\n",
    "    for pos in compiled_data_interp[strain].keys():\n",
    "        print('Warping {}, Position {}'.format(strain, pos))\n",
    "        for timepoint in timepoints:\n",
    "            # get warp from/to model at timepoint (the seam cells)\n",
    "            pos_seam_cells = compiled_data_interp[strain][pos]['seam_cells']\n",
    "            warp_to_seam_cell_names = warping_model['seam_cells']\n",
    "            warp_to_seam_cell_coords = warping_model['coordinates']\n",
    "            warp_to_at_timepoint = warp_to_seam_cell_coords[timepoint] # contains all warping data\n",
    "            \n",
    "            # however, we might not need/have all the data so here we match what we have\n",
    "            warp_to = []\n",
    "            warp_from = []\n",
    "            for warp_seam_cell_idx, warp_to_seam_cell_name in enumerate(warp_to_seam_cell_names):\n",
    "                if warp_to_seam_cell_name in pos_seam_cells.keys():\n",
    "                    # add to warp to\n",
    "                    warp_to_coords = warp_to_at_timepoint[warp_seam_cell_idx]\n",
    "                    warp_to.append(warp_to_coords)\n",
    "                    \n",
    "                    # add to warp from\n",
    "                    all_time_coords = pos_seam_cells[warp_to_seam_cell_name]['coordinates']\n",
    "                    warp_from_timepoint_coord = all_time_coords[timepoint]\n",
    "                    warp_from.append(warp_from_timepoint_coord)\n",
    "            \n",
    "            warp_to = np.array(warp_to)\n",
    "            warp_from = np.array(warp_from)\n",
    "            # print('from', np.array(warp_from))\n",
    "            # print('to', np.array(warp_to))\n",
    "            \n",
    "            # warp each cell at timepoint, including the seam cells themselves\n",
    "            for cell_type in ['seam_cells', 'annotations']:\n",
    "                # get cell order for warping\n",
    "                sorted_cell_names = compiled_data_interp[strain][pos][cell_type].keys()\n",
    "                ordered_coord_list = []\n",
    "                for cell_name in sorted_cell_names:\n",
    "                    old_coords = [compiled_data_interp[strain][pos][cell_type][cell_name]['coordinates'][timepoint]]\n",
    "                    old_coords = np.array(old_coords).flatten().tolist() # ensure correct data shape\n",
    "                    ordered_coord_list.append(old_coords)\n",
    "                ordered_coord_list = np.array(ordered_coord_list)\n",
    "                \n",
    "                # warping\n",
    "                \"\"\"\n",
    "                if cell_type == 'seam_cells':\n",
    "                    new_coords = thin_plate_spline_warp(warp_from, warp_to, ordered_coord_list)\n",
    "                else:\"\"\"\n",
    "                new_coords = thin_plate_spline_warp_c_elegans(warp_from, warp_to, ordered_coord_list)\n",
    "                new_coords = new_coords.tolist()\n",
    "                \n",
    "                # assigning\n",
    "                for cell_idx, cell_name in enumerate(sorted_cell_names):\n",
    "                    # if the old coordinates used to be 0 (ignored), then keep it that way\n",
    "                    old_coords = compiled_data_interp[strain][pos][cell_type][cell_name]['coordinates'][timepoint]\n",
    "                    \n",
    "                    if old_coords == [0, 0, 0]:\n",
    "                        assign_coords = old_coords\n",
    "                    else:\n",
    "                        assign_coords = new_coords[cell_idx]\n",
    "                        \n",
    "                    compiled_data_warped[strain][pos][cell_type][cell_name]['coordinates'][timepoint] = \\\n",
    "                        assign_coords\n",
    "    \n",
    "print('Step 5 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "output_json = compiled_data_warped\n",
    "compiled_json = json.dumps(output_json, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '5_compiled_data_warped.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 5 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Calculation Completed.\n",
      "Std_dev Calculation Completed.\n",
      "Sem Calculation Completed.\n",
      "Ci_95_lower Calculation Completed.\n",
      "Ci_95_upper Calculation Completed.\n",
      "Step 6 Completed.\n",
      "Step 6 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Reformat data to average points/calculate statistics\n",
    "step_data = copy.deepcopy(compiled_data_warped)\n",
    "data_by_cell = {'seam_cells':{}, 'annotations':{}}\n",
    "for strain in step_data.keys():\n",
    "    for pos in step_data[strain].keys():\n",
    "        for cell_type in ['seam_cells', 'annotations']:\n",
    "            for cell in step_data[strain][pos][cell_type].keys():\n",
    "                if cell not in data_by_cell[cell_type].keys():\n",
    "                    data_by_cell[cell_type][cell] = []\n",
    "\n",
    "                cell_coordindates = np.array(\n",
    "                    step_data[strain][pos][cell_type][cell]['coordinates'])\n",
    "                data_by_cell[cell_type][cell].append(\n",
    "                    cell_coordindates.tolist()) # should be putting full coordinates at end\n",
    "\n",
    "# perform operations/statistics on all coordinates by cell\n",
    "data_by_cell_stats = {'mean': {}, 'std_dev': {}, 'sem': {}, 'ci_95_lower': {}, 'ci_95_upper': {}}\n",
    "for calc_type in data_by_cell_stats.keys():\n",
    "    data_by_cell_stats[calc_type] = copy.deepcopy(data_by_cell)\n",
    "    \n",
    "    for cell_type in ['seam_cells', 'annotations']:\n",
    "        # seam cells should just result in the average warping model\n",
    "        for cell in data_by_cell_stats[calc_type][cell_type].keys():\n",
    "            cell_coords = np.array(data_by_cell_stats[calc_type][cell_type][cell])\n",
    "\n",
    "            if calc_type == 'mean': # average all the coordinates\n",
    "                data_by_cell_stats[calc_type][cell_type][cell] = np.average(cell_coords, axis=0).tolist()\n",
    "            elif calc_type == 'std_dev': # find standard deviation of all coordinates\n",
    "                data_by_cell_stats[calc_type][cell_type][cell] = np.std(cell_coords, axis=0).tolist()\n",
    "            elif calc_type == 'sem': # find standard error of all coordinates\n",
    "                data_by_cell_stats[calc_type][cell_type][cell] = st.sem(cell_coords, axis=0).tolist()\n",
    "            elif 'ci_95' in calc_type: # find 95% confidence interval\n",
    "                ci_ppf = st.t.ppf((1+0.95)/2, cell_coords.shape[0])\n",
    "                time_dim_mean = np.array(data_by_cell_stats['mean'][cell_type][cell])\n",
    "                time_dim_sem = np.array(data_by_cell_stats['sem'][cell_type][cell])\n",
    "                \n",
    "                if 'lower' in calc_type:\n",
    "                    ci_val = time_dim_mean - ci_ppf*time_dim_sem\n",
    "                elif 'upper' in calc_type:\n",
    "                    ci_val = time_dim_mean + ci_ppf*time_dim_sem\n",
    "                \n",
    "                data_by_cell_stats[calc_type][cell_type][cell] = ci_val.tolist()\n",
    "                        \n",
    "    print('{} Calculation Completed.'.format(calc_type.capitalize()))\n",
    "\n",
    "print('Step 6 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "output_json = data_by_cell_stats\n",
    "compiled_json = json.dumps(output_json, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '6_cell_coord_stats_by_timepoint.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 6 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 Completed.\n",
      "Step 7 Data Logged.\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Perform spatial moving average for each cell\n",
    "data_stats = copy.deepcopy(data_by_cell_stats)\n",
    "smoothing_window = config['settings']['smoothing']['window_size']\n",
    "\n",
    "# average all coordinates by cell and for all stats\n",
    "for calc_type in data_stats.keys():\n",
    "    for cell_type in ['seam_cells', 'annotations']:\n",
    "        # seam cells should just result in the average warping model\n",
    "        for cell in data_stats[calc_type][cell_type].keys():\n",
    "            # spatial average all the coordinates by time\n",
    "            coordinates = np.array(data_stats[calc_type][cell_type][cell])\n",
    "            # go through each axis\n",
    "            for dim_idx in range(3):\n",
    "                coord_dim_data = coordinates[:, dim_idx].copy()\n",
    "                data_smoothed = moving_average(coord_dim_data, smoothing_window)\n",
    "                coordinates[:, dim_idx] = data_smoothed\n",
    "\n",
    "            data_stats[calc_type][cell_type][cell] = coordinates.tolist()\n",
    "\n",
    "print('Step 7 Completed.')\n",
    "# save information as intermediate step in workspace\n",
    "output_json = data_stats\n",
    "compiled_json = json.dumps(output_json, sort_keys=True, indent=4)\n",
    "compiled_json_filepath = os.path.join(\n",
    "    workspace_folderpath, '7_cell_coord_stats_by_timepoint_smoothed.json')\n",
    "with open(compiled_json_filepath, \"w\") as f: \n",
    "    f.write(compiled_json)\n",
    "print('Step 7 Data Logged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 Completed.\n",
      "Output location: Y:\\RyanC\\model_building_code\\workspace\\2020_09_09-RW10752_NU\\output\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Convert to csv in MIPAV format\n",
    "scale = config['settings']['voxel_to_micron_scale']\n",
    "labels_on = config['settings']['mipav_output']['labels_on']\n",
    "# create timepoints array again just to be safe\n",
    "total_len = config['settings']['interpolation']['total_min'] # in minutes\n",
    "cell_info_file = config['settings']['mipav_output']['cell_info']\n",
    "cell_info = None\n",
    "with open(cell_info_file) as f:\n",
    "    cell_info = json.load(f)\n",
    "timepoints = np.arange(0, total_len)\n",
    "\n",
    "# generate place to put all the files\n",
    "output_folder = os.path.join(workspace_folderpath, 'output')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "                \n",
    "\n",
    "for cell_type in ['seam_cells', 'annotations']:\n",
    "    cells = data_stats['mean'][cell_type] # only output the mean\n",
    "\n",
    "    for cell_name in cells.keys():\n",
    "        # causes file issues + handle escape sequences\n",
    "        file_cell_name = cell_name.replace('/','_').replace(' ', '_')\n",
    "        filename_csv = file_cell_name +'.csv'\n",
    "        filepath_csv = os.path.join(output_folder, filename_csv)\n",
    "\n",
    "        coordinates = data_stats['mean'][cell_type][cell_name]\n",
    "        new_table = pd.DataFrame(coordinates) * scale[0]\n",
    "        new_table.columns = ['x', 'y', 'z']\n",
    "        new_table.insert(0, \"timepoints\", timepoints, True)\n",
    "        # colors\n",
    "        if cell_name.lower() in cell_info.keys():\n",
    "            color_timepoints = list(cell_info[cell_name.lower()]['colors'].keys())\n",
    "            colors = [cell_info[cell_name.lower()]['colors'][color_timepoint] for color_timepoint in color_timepoints]\n",
    "            colors = np.array(colors)\n",
    "            color_timepoints = np.array(color_timepoints).astype(float) # because originally string\n",
    "            \n",
    "            # color_timepoints_sorted_idx, color_timepoints_sorted = (np.argsort(color_timepoints), np.sort(color_timepoints))\n",
    "            # colors_sorted = np.array(colors[color_timepoints_sorted_idx]) # get sorted order\n",
    "            \n",
    "            # if there's one, just use nearest, otherwise use linear\n",
    "            if color_timepoints.shape[0] == 1: # use single value\n",
    "                placeholder_color = np.ones((total_len,))\n",
    "\n",
    "                for dim_idx, channel in enumerate(['R', 'G', 'B']):\n",
    "                    # color calculation/interpolation goes here\n",
    "                    dim_color = colors[0, dim_idx]\n",
    "                    interped_colors = placeholder_color * dim_color\n",
    "\n",
    "                    new_table.insert(dim_idx + 4, channel, interped_colors, True)\n",
    "                \n",
    "            else:\n",
    "                color_interp_method = 'linear'\n",
    "                for dim_idx, channel in enumerate(['R', 'G', 'B']):\n",
    "                    # color calculation/interpolation goes here\n",
    "                    interp = interpolate.interp1d(color_timepoints, colors[:, dim_idx], kind=color_interp_method)\n",
    "                    interped_colors = np.round(interp(timepoints))\n",
    "\n",
    "                    new_table.insert(dim_idx + 5, channel, interped_colors, True)\n",
    "                \n",
    "        else: # set color to white\n",
    "            placeholder_color = np.ones((total_len,)) * 255\n",
    "            new_table.insert(4, \"R\", placeholder_color, True)\n",
    "            new_table.insert(5, \"G\", placeholder_color, True)\n",
    "            new_table.insert(6, \"B\", placeholder_color, True)\n",
    "            \n",
    "        if labels_on:\n",
    "            new_table.insert(7, \"label\", np.ones((total_len,)), True)\n",
    "        else:\n",
    "            new_table.insert(7, \"label\", np.zeros((total_len,)), True)\n",
    "            \n",
    "        # format each column\n",
    "        for coord_col in ['x', 'y', 'z']: # as scientific notation\n",
    "            new_table[coord_col] = new_table[coord_col].map(lambda x: '%.6e' % x)\n",
    "            \n",
    "        for color_col in ['R', 'G', 'B', 'label']: # as integers\n",
    "            new_table[color_col] = new_table[color_col].map(lambda x: '%d' % x)\n",
    "\n",
    "        new_table.to_csv(filepath_csv, index=False, header=False)\n",
    "        \n",
    "full_output_folderpath = os.path.join(os.getcwd(), output_folder)\n",
    "print('Step 8 Completed.')\n",
    "print('Output location: {}'.format(full_output_folderpath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
